{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AdaInTrain.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1g7FpfDKIxn_GNPL0fWBIWq5JafKPmn-8","authorship_tag":"ABX9TyO7kUEwpt+QV++59Y89pJ7N"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"wlF4iFcx4E09","executionInfo":{"status":"ok","timestamp":1624434093588,"user_tz":-120,"elapsed":2402,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["import os \n","import tensorflow as tf\n","from tensorflow.keras.applications import vgg19, VGG19\n","from tensorflow.keras.layers import Conv2D, UpSampling2D"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"znsfY-SD6P7q"},"source":["Archittettura della rete neurale per il trasferimento di stile come descritta nell'articolo"]},{"cell_type":"code","metadata":{"id":"r89TOCsx5tLU","executionInfo":{"status":"ok","timestamp":1624434093591,"user_tz":-120,"elapsed":39,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["class ReflectionPadding2D(tf.keras.layers.Layer):\n","  \n","    def __init__(self, padding=1, **kwargs):\n","        super(ReflectionPadding2D, self).__init__(**kwargs)\n","        self.padding = padding\n","\n","    def compute_output_shape(self, s):\n","        return s[0], s[1] + 2 * self.padding, s[2] + 2 * self.padding, s[3]\n","\n","    def call(self, x):\n","        return tf.pad(\n","            x,\n","            [\n","                [0, 0],\n","                [self.padding, self.padding],\n","                [self.padding, self.padding],\n","                [0, 0],\n","            ],\n","            \"REFLECT\",\n","        )\n","\n","\n","class TransferNet(tf.keras.Model):\n","    def __init__(self, content_layer):\n","        super(TransferNet, self).__init__()\n","        self.encoder = Encoder(content_layer)\n","        self.decoder = decoder()\n","\n","    def encode(self, content_image, style_image, alpha):\n","        content_feat = self.encoder(content_image)\n","        style_feat = self.encoder(style_image)\n","\n","        t = adaptive_instance_normalization(content_feat, style_feat)\n","        t = alpha * t + (1 - alpha) * content_feat\n","        return t\n","\n","    def decode(self, t):\n","        return self.decoder(t)\n","\n","    def call(self, content_image, style_image, alpha=1.0):\n","        t = self.encode(content_image, style_image, alpha)\n","        g_t = self.decode(t)\n","        return g_t\n","\n","\n","def adaptive_instance_normalization(content_feat, style_feat, epsilon=1e-5):\n","    content_mean, content_variance = tf.nn.moments(\n","        content_feat, axes=[1, 2], keepdims=True\n","    )\n","    style_mean, style_variance = tf.nn.moments(\n","        style_feat, axes=[1, 2], keepdims=True\n","    )\n","    style_std = tf.math.sqrt(style_variance + epsilon)\n","\n","    norm_content_feat = tf.nn.batch_normalization(\n","        content_feat,\n","        mean=content_mean,\n","        variance=content_variance,\n","        offset=style_mean,\n","        scale=style_std,\n","        variance_epsilon=epsilon,\n","    )\n","    return norm_content_feat\n","\n","\n","class Encoder(tf.keras.models.Model):\n","    def __init__(self, content_layer):\n","        super(Encoder, self).__init__()\n","        vgg = VGG19(include_top=False, weights=\"imagenet\")\n","\n","        self.vgg = tf.keras.Model(\n","            [vgg.input], [vgg.get_layer(content_layer).output]\n","        )\n","        self.vgg.trainable = False\n","\n","    def call(self, inputs):\n","        preprocessed_input = vgg19.preprocess_input(inputs)\n","        return self.vgg(preprocessed_input)\n","\n","\n","def decoder():\n","    return tf.keras.Sequential(\n","        [\n","            ReflectionPadding2D(),\n","            Conv2D(256, (3, 3), activation=\"relu\"),\n","            UpSampling2D(size=2),\n","            ReflectionPadding2D(),\n","            Conv2D(256, (3, 3), activation=\"relu\"),\n","            ReflectionPadding2D(),\n","            Conv2D(256, (3, 3), activation=\"relu\"),\n","            ReflectionPadding2D(),\n","            Conv2D(256, (3, 3), activation=\"relu\"),\n","            ReflectionPadding2D(),\n","            Conv2D(128, (3, 3), activation=\"relu\"),\n","            UpSampling2D(size=2),\n","            ReflectionPadding2D(),\n","            Conv2D(128, (3, 3), activation=\"relu\"),\n","            ReflectionPadding2D(),\n","            Conv2D(64, (3, 3), activation=\"relu\"),\n","            UpSampling2D(size=2),\n","            ReflectionPadding2D(),\n","            Conv2D(64, (3, 3), activation=\"relu\"),\n","            ReflectionPadding2D(),\n","            Conv2D(3, (3, 3)),\n","        ]\n","    )\n","\n","\n","class VGG(tf.keras.models.Model):\n","    def __init__(self, content_layer, style_layers):\n","        super(VGG, self).__init__()\n","        vgg = VGG19(include_top=False, weights=\"imagenet\")\n","\n","        content_output = vgg.get_layer(content_layer).output\n","        style_outputs = [vgg.get_layer(name).output for name in style_layers]\n","\n","        self.vgg = tf.keras.Model(\n","            [vgg.input], [content_output, style_outputs]\n","        )\n","        self.vgg.trainable = False\n","\n","    def call(self, inputs):\n","        preprocessed_input = vgg19.preprocess_input(inputs)\n","        content_outputs, style_outputs = self.vgg(preprocessed_input)\n","        return content_outputs, style_outputs\n"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E7v8_5L5_I-i"},"source":["Variabili di controllo"]},{"cell_type":"code","metadata":{"id":"HC0_rSIV_O32","executionInfo":{"status":"ok","timestamp":1624434093594,"user_tz":-120,"elapsed":36,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["logDir = '/content/drive/MyDrive/UpdatedModel0'        # cartella in cui si trova il modello da addestrare (da zero o da un modello parzialmente addestrato)\n","lr =1e-4           #learning-rate\n","lrDecay = 5e-5     #learning rate Decay\n","imageSize= 256     # dimensione delle immagini utilizzate per il \"random crop\"\n","batchSize = 8\n","contentWeight = 1\n","styleWeight = 10\n","logFreq = 50 #500      # frequenza con cui vengono stampate le informazioni del training"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ji0uLRhFA48J","executionInfo":{"status":"ok","timestamp":1624434093598,"user_tz":-120,"elapsed":39,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["content_layer = \"block4_conv1\"  # relu-4-1\n","\n","style_layers = [\n","        \"block1_conv1\",  # relu1-1\n","        \"block2_conv1\",  # relu2-1\n","        \"block3_conv1\",  # relu3-1\n","        \"block4_conv1\",  # relu4-1\n","    ]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHE1MxupBOST","executionInfo":{"status":"ok","timestamp":1624434095389,"user_tz":-120,"elapsed":1822,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["vgg = VGG(content_layer, style_layers)\n","transformer = TransferNet(content_layer)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_8v2IxYyCtBA"},"source":["Funzioni ausiliarie per modificare le immagini nel dataset.  \n","Le immagini verranno ridimensionate e per eseguire il training verra' scelta una regione casuale di dimensione casuale specificata dalla variabile imageSize."]},{"cell_type":"code","metadata":{"id":"hH96d-TyFVGf","executionInfo":{"status":"ok","timestamp":1624434095392,"user_tz":-120,"elapsed":26,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["def resize(img, min_size=512):\n","    \"\"\" Resize image and keep aspect ratio \"\"\"\n","    width, height, _ = tf.unstack(tf.shape(img), num=3)\n","    if height < width:\n","        new_height = min_size\n","        new_width = int(width * new_height / height)\n","    else:\n","        new_width = min_size\n","        new_height = int(height * new_width / width)\n","\n","    img = tf.image.resize(img, size=(new_width, new_height))\n","    return img\n","\n","def resize_and_crop(img, min_size):\n","    img = resize(img, min_size=min_size)\n","    img = tf.image.random_crop(img, size=(imageSize, imageSize, 3))\n","    img = tf.cast(img, tf.float32)\n","    return img\n","\n","def process_content(file_path):\n","    #img = features[\"image\"]\n","    #img = resize_and_crop(img, min_size=286)\n","    img = tf.io.read_file(file_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = resize_and_crop(img, min_size=286)\n","    return img\n","\n","def process_style(file_path):\n","    img = tf.io.read_file(file_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = resize_and_crop(img, min_size=512)\n","    return img"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFxR3EacUUht"},"source":["E' necessario dividere il dataset del contenuto e quello dello stile in piÃ¹ cartelle in quanto colab non riesce a leggere molti file in una sola volta.  \n","Successivamente si uniscono insieme tutti i dataset in uno unico."]},{"cell_type":"code","metadata":{"id":"IUIGiS-UJjCV","executionInfo":{"status":"ok","timestamp":1624434105079,"user_tz":-120,"elapsed":9707,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","# 4 dataset per il contenuto\n","contentDatasetPath1 = '/content/drive/MyDrive/DatesetContenuto/train2014_20000/001'\n","contentDatasetPath2 = '/content/drive/MyDrive/DatesetContenuto/train2014_20000/002'\n","contentDatasetPath3 = '/content/drive/MyDrive/DatesetContenuto/train2014_20000/003'\n","contentDatasetPath4 = '/content/drive/MyDrive/DatesetContenuto/train2014_20000/004'\n","\n","#4 dataset per lo stile\n","\n","styleDatasetPath1 = '/content/drive/MyDrive/DatasetStile/train_20000/001'\n","styleDatasetPath2 = '/content/drive/MyDrive/DatasetStile/train_20000/002'\n","styleDatasetPath3 = '/content/drive/MyDrive/DatasetStile/train_20000/003'\n","styleDatasetPath4 = '/content/drive/MyDrive/DatasetStile/train_20000/004'\n","\n","\n","\n","ds_coco = (\n","    tf.data.Dataset.list_files(os.path.join(contentDatasetPath1, \"*.jpg\"))\n","    .concatenate(tf.data.Dataset.list_files(os.path.join(contentDatasetPath2, \"*.jpg\")))\n","    .concatenate(tf.data.Dataset.list_files(os.path.join(contentDatasetPath3, \"*.jpg\")))\n","    .concatenate(tf.data.Dataset.list_files(os.path.join(contentDatasetPath4, \"*.jpg\")))\n","    .map(process_content, num_parallel_calls=AUTOTUNE)\n","    # Ignore too large or corrupt image files\n","    .apply(tf.data.experimental.ignore_errors()) \n","    .repeat()\n","    .batch(batchSize)\n","    .prefetch(AUTOTUNE)\n","    \n",")\n","\n","ds_pbn = (\n","    tf.data.Dataset.list_files(os.path.join(styleDatasetPath1, \"*.jpg\"))\n","    .concatenate(tf.data.Dataset.list_files(os.path.join(styleDatasetPath2, \"*.jpg\")))\n","    .concatenate(tf.data.Dataset.list_files(os.path.join(styleDatasetPath3, \"*.jpg\")))\n","    .concatenate(tf.data.Dataset.list_files(os.path.join(styleDatasetPath4, \"*.jpg\")))\n","    .map(process_style, num_parallel_calls=AUTOTUNE)\n","    # Ignore too large or corrupt image files\n","    .apply(tf.data.experimental.ignore_errors())\n","    .repeat()\n","    .batch(batchSize)\n","    .prefetch(AUTOTUNE)\n","    \n",")\n","\n","\n","ds = tf.data.Dataset.zip((ds_coco, ds_pbn))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ix1MWSrBPqGC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624434105086,"user_tz":-120,"elapsed":27,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}},"outputId":"8889c00c-6608-4be4-e258-57b725873854"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","print(optimizer.learning_rate)\n","ckpt = tf.train.Checkpoint(optimizer=optimizer, transformer=transformer)\n","\n","manager = tf.train.CheckpointManager(ckpt, logDir, max_to_keep=1)\n","ckpt.restore(manager.latest_checkpoint).expect_partial()\n","print(optimizer.learning_rate)\n","if manager.latest_checkpoint:\n","    print(f\"Restored from {manager.latest_checkpoint}\")\n","else:\n","    print(\"Initializing from scratch.\")\n","\n","summary_writer = tf.summary.create_file_writer(logDir)\n","\n","train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n","train_style_loss = tf.keras.metrics.Mean(name=\"train_style_loss\")\n","train_content_loss = tf.keras.metrics.Mean(name=\"train_content_loss\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=1e-04>\n","<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=9.975062e-05>\n","Restored from /content/drive/MyDrive/UpdatedModel0/ckpt-114\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gRluSZl8-vz_"},"source":["Funzioni di perdita "]},{"cell_type":"code","metadata":{"id":"WQJ13G4n-x_A","executionInfo":{"status":"ok","timestamp":1624434105087,"user_tz":-120,"elapsed":18,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["def mean_std_loss(feat, feat_stylized, epsilon=1e-5):\n","    feat_mean, feat_variance = tf.nn.moments(feat, axes=[1, 2])\n","    feat_stylized_mean, feat_stylized_variance = tf.nn.moments(\n","        feat_stylized, axes=[1, 2]\n","    )\n","    feat_std = tf.math.sqrt(feat_variance + epsilon)\n","    feat_stylized_std = tf.math.sqrt(feat_stylized_variance + epsilon)\n","\n","    loss = tf.losses.mse(feat_stylized_mean, feat_mean) + tf.losses.mse(\n","        feat_stylized_std, feat_std\n","    )\n","    return loss\n","\n","def style_loss(feat, feat_stylized):\n","    return tf.reduce_sum(\n","        [\n","            mean_std_loss(f, f_stylized)\n","            for f, f_stylized in zip(feat, feat_stylized)\n","        ]\n","    )\n","\n","\n","def content_loss(feat, feat_stylized):\n","    return tf.reduce_mean(tf.square(feat - feat_stylized), axis=[1, 2, 3])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1Rq3iNWRV1d","executionInfo":{"status":"ok","timestamp":1624434105088,"user_tz":-120,"elapsed":17,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}}},"source":["@tf.function\n","def train_step(content_img, style_img):\n","    t = transformer.encode(content_img, style_img, alpha=1.0)\n","\n","    with tf.GradientTape() as tape:\n","        stylized_img = transformer.decode(t)\n","\n","        _, style_feat_style = vgg(style_img)\n","        content_feat_stylized, style_feat_stylized = vgg(stylized_img)\n","\n","        tot_style_loss = styleWeight * style_loss(style_feat_style, style_feat_stylized)\n","        tot_content_loss = contentWeight * content_loss(t, content_feat_stylized)\n","        loss = tot_style_loss + tot_content_loss\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)\n","    train_style_loss(tot_style_loss)\n","    train_content_loss(tot_content_loss)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PH5x4q6CShes","outputId":"38e724d0-bc2a-403d-8605-ec1e1f530884"},"source":["for step, (content_images, style_images) in enumerate(ds):\n","    new_lr = lr / (1.0 + lrDecay * step )\n","    optimizer.learning_rate.assign(new_lr)\n","\n","    train_step(content_images, style_images)\n","\n","    if step % logFreq == 0:\n","        with summary_writer.as_default():\n","            tf.summary.scalar(\"loss/total\", train_loss.result(), step=step)\n","            tf.summary.scalar(\"loss/style\", train_style_loss.result(), step=step)\n","            tf.summary.scalar(\"loss/content\", train_content_loss.result(), step=step)\n","            print(\n","                f\"Step {step}, \"\n","                f\"Loss: {train_loss.result()}, \"\n","                f\"Style Loss: {train_style_loss.result()}, \"\n","                f\"Content Loss: {train_content_loss.result()}\"\n","            )\n","            print(f\"Saved checkpoint: {manager.save()}\")\n","\n","            train_loss.reset_states()\n","            train_style_loss.reset_states()\n","            train_content_loss.reset_states()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Step 0, Loss: 3648751.25, Style Loss: 3191310.25, Content Loss: 457441.03125\n","Saved checkpoint: /content/drive/MyDrive/UpdatedModel0/ckpt-115\n","Step 50, Loss: 3552482.0, Style Loss: 3100457.5, Content Loss: 452024.6875\n","Saved checkpoint: /content/drive/MyDrive/UpdatedModel0/ckpt-116\n","Step 100, Loss: 3159343.0, Style Loss: 2724058.5, Content Loss: 435284.3125\n","Saved checkpoint: /content/drive/MyDrive/UpdatedModel0/ckpt-117\n","Step 150, Loss: 3525057.5, Style Loss: 3066285.0, Content Loss: 458771.96875\n","Saved checkpoint: /content/drive/MyDrive/UpdatedModel0/ckpt-118\n","Step 200, Loss: 3633692.25, Style Loss: 3174478.5, Content Loss: 459213.84375\n","Saved checkpoint: /content/drive/MyDrive/UpdatedModel0/ckpt-119\n","Step 250, Loss: 3200373.75, Style Loss: 2753378.25, Content Loss: 446995.46875\n","Saved checkpoint: /content/drive/MyDrive/UpdatedModel0/ckpt-120\n"],"name":"stdout"}]}]}
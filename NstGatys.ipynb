{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NstGatys.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNlsUq95iiehMXxi9ot4YKZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HIs4Ovh-uMM3"},"source":["Implementazione dell'algoritmo per il trasferimento di stile descritto nell' articolo: [Image Style Transfer Using Convolutional Neural Networks](https://ieeexplore.ieee.org/document/7780634)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4tgmoaRvCS5","executionInfo":{"status":"ok","timestamp":1624007841641,"user_tz":-120,"elapsed":227,"user":{"displayName":"Andrea Baroni","photoUrl":"","userId":"13306789850142911213"}},"outputId":"9d40f21d-ee6f-499a-d83d-822a568a1779"},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.applications import vgg19\n","\n","'''base_image_path = keras.utils.get_file(\"paris.jpg\", \"https://i.imgur.com/F28w3Ac.jpg\")\n","style_reference_image_path = keras.utils.get_file(\n","    \"starry_night.jpg\", \"https://i.imgur.com/9ooB60I.jpg\"\n",")'''\n","# base_image_path rappresenta il contenuto dell'immagine che si vuole andare a sintetizzare\n","base_image_path = \"/content/img1.jpg\"\n","\n","# style_reference_image_path rappresenta lo stile dell'immagine che si vuole sintetizzare\n","style_reference_image_path =\"/content/sketch_cropped.png\"\n","\n","result_prefix = \"result\"\n","\n","# vengono settati i pesi per le differenti funzioni di perdita\n","total_variation_weight = 1e-3\n","style_weight = 4e-0\n","content_weight = 4e-3\n","\n","# Dimensione dell'immagine che si andrà a generare .\n","width, height = keras.preprocessing.image.load_img(base_image_path).size\n","img_nrows = 400\n","# vengono mantenute le proporzioni\n","img_ncols = int(width * img_nrows / height) \n","print(base_image_path,img_nrows,img_ncols)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/img1.jpg 400 416\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iGejCLqR0G5y"},"source":["Prima di inserire le immagini all'interno della rete vgg19 è necessario che siano pre - processate e forzate ad avere una certa dimensione."]},{"cell_type":"code","metadata":{"id":"BHC_5R4mvCTG"},"source":["def preprocess_image(image_path):    \n","    img = keras.preprocessing.image.load_img(\n","        image_path, target_size=(img_nrows, img_ncols)\n","    )\n","    img = keras.preprocessing.image.img_to_array(img)\n","    # viene aggiunta la dimensione per il batch\n","    img = np.expand_dims(img, axis=0) \n","    img = vgg19.preprocess_input(img)\n","    return tf.convert_to_tensor(img)\n","\n","#funzione che si occupa di eseguire i passi inversi della funzione \n","def deprocess_image(x):\n","    # Util function to convert a tensor into a valid image\n","    x = x.reshape((img_nrows, img_ncols, 3))\n","    # Remove zero-center by mean pixel\n","    x[:, :, 0] += 103.939\n","    x[:, :, 1] += 116.779\n","    x[:, :, 2] += 123.68\n","    # 'BGR'->'RGB'\n","    x = x[:, :, ::-1]\n","    x = np.clip(x, 0, 255).astype(\"uint8\")\n","    return x\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wr7bi2ZR86BU"},"source":["# Funzioni di perdita\n","Vengono definite le funzioni di perdita come descritto dall'articolo originario."]},{"cell_type":"code","metadata":{"id":"WP0d737AvCTL"},"source":["# La matrice di gram viene impiegata per calcolare la funzione di perdita che riguarda lo stile: \"style_loss\"\n","\n","def gram_matrix(x):\n","    x = tf.transpose(x, (2, 0, 1))\n","    features = tf.reshape(x, (tf.shape(x)[0], -1))\n","    gram = tf.matmul(features, tf.transpose(features))\n","    return gram\n","\n","\n","# Funzione di perdita per quanto riguarda lo stile:\n","# viene calcolata la matrice si gram in un particolare layer\n","# sia per l'immagine per cui si vuole imitare lo stile che per l'immagine\n","# che si sta andando a sintetizzare \n","\n","def style_loss(style, combination):\n","    S = gram_matrix(style)\n","    C = gram_matrix(combination)\n","    channels = np.size(S,0)\n","    #channels = 3\n","    size = img_nrows * img_ncols\n","    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n","\n","\n","# Funzione di perdita per calcolare lo scostamento tra il contenuto  \n","def content_loss(base, combination):\n","    return tf.reduce_sum(tf.square(combination - base))\n","\n","\n","# Funzione ausiliaria per ridurre il rumore\n","def total_variation_loss(x):\n","    a = tf.square(\n","        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, 1:, : img_ncols - 1, :]\n","    )\n","    b = tf.square(\n","        x[:, : img_nrows - 1, : img_ncols - 1, :] - x[:, : img_nrows - 1, 1:, :]\n","    )\n","    return tf.reduce_sum(tf.pow(a + b, 1.25))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t4FrDH4UJf6w"},"source":["import numpy as np\n","from PIL import Image\n","\n","arr = np.random.rand(400,400,3) * 255\n","arr.clip(0,255).astype(\"uint8\")\n","keras.preprocessing.image.save_img(\"p.jpg\", arr.clip(0,255).astype(\"uint8\"))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qOX7vUbuwboM"},"source":["# per estrarre il contenuto e lo stile di un'immagine\n","# viene impiegata la rete vgg19, include_top = False non include \n","# gli ultimi livelli totalmente connessi\n","model = vgg19.VGG19(weights=\"imagenet\", include_top=False)\n","\n","# viene crato un dizionario in quanto quando verra' creato il modello\n","# si potra' recupare i layer necessari per ricostruire il contenuto e lo \n","# stile \n","outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n","\n","feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVZpg7G-_dm2"},"source":["# Lista di layer impiegati per calcolare la funzione di perdita dello stile\n","style_layer_names = [\n","    \"block1_conv1\",\n","    \"block2_conv1\",\n","    \"block3_conv1\",\n","    \"block4_conv1\",\n","    \"block5_conv1\",\n","]\n","# Layer impiegato per calcolare la perdita riguardo al contenuto .\n","content_layer_name = \"block5_conv2\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hO6Ex02DEfyC"},"source":["# funzione impiegata per calcolare la varie funzioni di perdita\n","# combination_image e' l'immagine di partenza che viene modellata per\n","# assumere lo stile dell'immagine \"style_reference_image\"\n","# e il contenuto di \"base_image\"\n","def compute_loss(combination_image, base_image, style_reference_image):\n","    input_tensor = tf.concat(\n","        [base_image, style_reference_image, combination_image], axis=0\n","    )\n","    #features e' un dizionario\n","    features = feature_extractor(input_tensor)\n","\n","    \n","    loss = tf.zeros(shape=())\n","\n","    # tramite il dizionario \"features\" viene estratto \n","    # l'output del layer responsabile del contenuto \n","    layer_features = features[content_layer_name]\n","    # il primo asse di layer_features indica il batch come definito \n","    # dalla variabile input_tensor\n","    base_image_features = layer_features[0, :, :, :]\n","    combination_features = layer_features[2, :, :, :]\n","    loss = loss + content_weight * content_loss(\n","        base_image_features, combination_features\n","    )\n","     \n","    for layer_name in style_layer_names:\n","        layer_features = features[layer_name]\n","        style_reference_features = layer_features[1, :, :, :]\n","        combination_features = layer_features[2, :, :, :]\n","        sl = style_loss(style_reference_features, combination_features)\n","        loss += (style_weight / len(style_layer_names)) * sl\n","\n","    # Si aggiunge anche una funzione di perdita per normalizzare l'immagine\n","    loss += total_variation_weight * total_variation_loss(combination_image)\n","    return loss\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eR8M7AF3NERZ"},"source":["#funzione impiegata per calcolare il gradiente della funzione \n","# di perdita\n","@tf.function\n","def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n","    with tf.GradientTape() as tape:\n","        loss = compute_loss(combination_image, base_image, style_reference_image)\n","    grads = tape.gradient(loss, combination_image)\n","    return loss, grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocTnAmukNyyU"},"source":["'''optimizer = keras.optimizers.SGD(\n","    keras.optimizers.schedules.ExponentialDecay(\n","        initial_learning_rate=1.0, decay_steps=4000, decay_rate=0.96\n","    )\n",")'''\n","\n","optimizer = tf.optimizers.Adam(learning_rate=5, beta_1=0.99, epsilon=1e-1)\n","\n","base_image = preprocess_image(base_image_path)\n","style_reference_image = preprocess_image(style_reference_image_path)\n","combination_image = tf.Variable(preprocess_image(\"/content/p.jpg\"))\n","print(\"numero assi combination: \",combination_image.shape)\n","iterations = 50000\n","for i in range(1, iterations + 1):\n","    loss, grads = compute_loss_and_grads(\n","        combination_image, base_image, style_reference_image\n","    )\n","    print(\"numero assi grad: \",grads.shape)\n","    optimizer.apply_gradients([(grads, combination_image)])\n","\n","    print(\"Iteration %d: loss=%.2f\" % (i, loss))\n","    if i % 200 == 0:\n","        print(\"Iteration %d: loss=%.2f\" % (i, loss))\n","        img = deprocess_image(combination_image.numpy())\n","        fname = result_prefix + \"_at_iteration_%d.png\" % i\n","        keras.preprocessing.image.save_img(fname, img)"],"execution_count":null,"outputs":[]}]}